{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as k\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if k.backend() == 'tensorflow':\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedBatchNormalization(Layer):\n",
    "    \n",
    "    def __init__(self, epsilon=1e-3, axis=-1,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.axis = axis\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        super(FixedBatchNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis], )\n",
    "        \n",
    "        self.gamma = self.add_weight(shape,\n",
    "                                     initializer = self.gamma_init,\n",
    "                                     regularizer = self.gamma_regularizer,\n",
    "                                     name = '{}_gamma'.format(self.name),\\\n",
    "                                     trainable = False)\n",
    "        self.beta = self.add_weight(shape,\n",
    "                                    initializer = self.beta_init,\n",
    "                                    regularizer = self.beta_regularizer,\n",
    "                                    name = '{}_beta'.format(self.name),\n",
    "                                    trainable = False)\\\n",
    "        \n",
    "        self.running_mean = self.add_weight(shape,\n",
    "                                           initializer = 'zero',\n",
    "                                           name = '{}_running_mean'.format(self.name),\n",
    "                                           trainable = False)\n",
    "        \n",
    "        self.running_std = self.add_weight(shape,\n",
    "                                           initializer = 'zero',\n",
    "                                          name = '{}_running_std'.format(self.name),\n",
    "                                          trainable = False)\n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        assert self.built, 'Layer must be built before being called'\n",
    "        input_shape = K.int_shape(x)\n",
    "        \n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "        \n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "        \n",
    "        if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "            x_normed = K.batch_normalization(\n",
    "                x, self.running_mean, self.running_std,\n",
    "                self.beta, self.gamma, epsilon=self.epsilon)\n",
    "            \n",
    "        else:\n",
    "            # need  broadcasting\n",
    "            broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "            broadcast_running_std  = K.reshape(self.running_std, broadcast_shape)\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            x_normed = K.batch_normalization(\n",
    "                x, broadcast_running_mean, broadcast_running_std,\n",
    "                broadcast_beta, broadcast_gamma, epsilon=self.epsilon)\n",
    "            \n",
    "        return x_normed\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None}\n",
    "        \n",
    "        base_config = super(FixedBatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "    def __init__(self , pool_size , num_rois , **kwargs ):\n",
    "        self.dim_ordering = k.image_dim_ordering()\n",
    "        self.pool_size =  pool_size\n",
    "        self.num_rois = num_rois\n",
    "        super(RoiPoolingConv , self ).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape ):\n",
    "        self.nb_channels = input_shape[0][3]\n",
    "        \n",
    "    \n",
    "    def compute_output_shape( self, input_shape ):\n",
    "        return ( None , self.num_rois , self.pool_size , self.pool_size, self.nb_channels )\n",
    "    \n",
    "    \n",
    "    def call( self , x ,  mask = None ):\n",
    "        img  =  x[0]\n",
    "        rois =  x[1]\n",
    "        input_shape = k.shape(img)\n",
    "        outputs = []\n",
    "        \n",
    "        for roi_idx in range( self.rois ):\n",
    "            x = rois[0 , roi_idx , 0 ]\n",
    "            y = rois[0 , roi_idx , 1 ]\n",
    "            w = rois[0 , roi_idx , 2 ]\n",
    "            h = rois[0 , roi_idx , 3 ]\n",
    "            \n",
    "            row_length = w / float(self.pool_size)\n",
    "            col_length = h / float(self.pool_size)\n",
    "            num_pools_regions = self.pool_size\n",
    "            \n",
    "            x = k.cast( x , 'int32')\n",
    "            y = k.cast( y , 'int32')\n",
    "            w = k.cast( w , 'int32')\n",
    "            h = k.cast( h , 'int32')\n",
    "            rs = tf.image.resize_images( img[: , y + y+h , x : x+w , :], (self.pool_size,self.pool_size))\n",
    "            outputs.append(rs)\n",
    "            \n",
    "        final_output = k.concatenate(outputs , axis=0)\n",
    "        final_output = k.reshape(final_output , ( 1 , self.num_rois , self.pool_size, self.pool_size,self.nb_channels))\n",
    "        final_output = k.permute_dimensions(final_output , ( 0 , 1 , 2 , 3 , 4 ))\n",
    "        return final_output\n",
    "    \n",
    "    def get_config( self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "            \n",
    "            \n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_output_length( width , height ):\n",
    "    \n",
    "    def get_output_length( input_length ):\n",
    "        return input_length/16\n",
    "    \n",
    "    return get_output_length(width) , get_output_length(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_base( input_tensor = None , trainable = False ):\n",
    "    input_shape = ( None , None ,  3)\n",
    "    \n",
    "    if input_tensor is None :\n",
    "        img_input = Input(shape = input_shape)\n",
    "    else:\n",
    "        if not k.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor , shape = input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "            \n",
    "    bn_axis = 1\n",
    "    # Bloack 1\n",
    "    x = Conv2D( 64 , ( 3 , 3) , activation= 'relu' , padding = 'same' , name = 'block1_conv1')(img_input)\n",
    "    x = Conv2D( 64 , ( 3 , 3) , activation= 'relu' , padding = 'same' , name = 'block1_conv2')(x)\n",
    "    x = MaxPooling2D( ( 2 , 2 ) , strides=( 2 ,  2) , name ='bloack1_pool')(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPN( base_layers , num_anchors ):\n",
    "    \n",
    "    x  = Conv2D(512 , ( 3,3) , padding = 'same' , activation = 'relu' , kernel_initializer='normal' , name = 'rpn_conv1')(base_layers)\n",
    "    # apply regression and classifier\n",
    "    x_class = Conv2D(num_anchors , (1 , 1) , activation='sigmoid' , kernel_initializer='uniform' , name = 'rpn_out_class')(x)\n",
    "    r_regr = Conv2D(num_anchors*4 , (1 , 1) , activation='linear' , kernel_initializer='zero' , name = 'rpn_out_regress')(x)    \n",
    "    return [x_class , r_regr , base_layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier( base_layers , input_rois , num_rois , nb_classess = 21 , trainable =  False):\n",
    "    pooling_regions =  7\n",
    "    input_shape = (num_rois , 7 , 7 , 512 )\n",
    "    \n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions , num_rois )([base_layers , input_rois])\n",
    "    \n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
